# Docling RAG

A completely **free, local RAG (Retrieval-Augmented Generation)** system that answers questions about your PDFs, including scanned documents with automatic OCR.

## ğŸ¯ Features

- ğŸ“„ **PDF Processing**: Text extraction + automatic OCR for scanned documents
- ğŸ” **Semantic Search**: Find relevant content by meaning, not just keywords
- ğŸ¤– **Local LLM**: Private, offline AI powered by Ollama (no API costs)
- âš¡ **Fast**: FAISS vector database for instant retrieval
- ğŸ’° **100% Free**: No subscriptions, no API calls, no cloud services
- ğŸ”’ **Privacy**: All data stays on your machine

## ğŸ—ï¸ Architecture

```
PDFs â†’ Docling (OCR) â†’ SentenceTransformers (embeddings) â†’ FAISS (vector DB) â†’ Ollama/Phi-3 â†’ Answers
```

## ğŸ“¦ Tech Stack

- **Docling**: PDF processing with automatic OCR
- **SentenceTransformers**: Semantic embeddings
- **FAISS**: Vector similarity search
- **Ollama + Phi-3**: Local LLM inference
- **Rich**: Beautiful terminal UI

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.10+**
2. **Ollama** - [Download here](https://ollama.com)

### Installation

```bash
# Clone the repository
git clone https://github.com/Supl3x/Docling-RAG.git
cd Docling-RAG

# Install dependencies
pip install -r requirements.txt

# Install Ollama model
ollama pull phi3
```

### Usage

```bash
# 1. Add PDFs to data/pdfs/ folder

# 2. Run the application
python app.py

# 3. First time: Choose "1" to ingest PDFs
# 4. Then: Choose "2" to chat with your documents
```

## ğŸ“ Project Structure

```
DOCLING RAG/
â”œâ”€â”€ data/pdfs/          # Your PDFs go here
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py       # PDF â†’ Embeddings â†’ Index
â”‚   â”œâ”€â”€ retrieve.py     # Vector search engine
â”‚   â””â”€â”€ chat.py         # LLM interaction
â”œâ”€â”€ index/              # Vector database (auto-created)
â”œâ”€â”€ app.py              # Main application
â””â”€â”€ requirements.txt    # Dependencies
```

## ğŸ’¡ How It Works

### Phase 1: Ingestion
1. Place PDFs in `data/pdfs/`
2. Docling extracts text (runs OCR if needed)
3. Text is chunked into semantic pieces
4. Chunks are converted to embeddings (vectors)
5. FAISS index is built for fast search

### Phase 2: Q&A (RAG)
1. User asks a question
2. Question is converted to embedding
3. FAISS finds most similar document chunks
4. Context + Question sent to local LLM
5. LLM generates answer using only the context
6. Answer displayed with source citations

## ğŸ“ What You'll Learn

- Document processing & OCR
- Vector embeddings & semantic search
- RAG (Retrieval-Augmented Generation)
- Vector databases (FAISS)
- Local LLM deployment
- System architecture & design

## ğŸ”§ Configuration

Edit these parameters in the code:

**Embeddings** (`src/ingest.py`, `src/retrieve.py`):
- Model: `all-MiniLM-L6-v2` (fast, 384-dim)
- Chunk size: 500 characters

**LLM** (`src/chat.py`):
- Model: `phi3` (2.2GB, efficient)
- Temperature: 0.1 (focused answers)
- Max tokens: 500

**Search** (`src/retrieve.py`):
- Top-K results: 5 chunks

## ğŸ“Š Performance

On a typical laptop:
- **OCR**: 2-5 seconds/page
- **Embedding**: 100-500ms/chunk
- **Search**: <50ms
- **LLM response**: 2-10 seconds

## ğŸ†š Cost Comparison

| Solution | Our System | Commercial (GPT-4 + Pinecone) |
|----------|------------|------------------------------|
| Setup | Free | Free |
| Monthly | **$0** | $90+ |
| Privacy | Local | Cloud |
| Offline | âœ… Yes | âŒ No |

## ğŸ› ï¸ Troubleshooting

**No PDFs found**: Add `.pdf` files to `data/pdfs/`

**OCR fails**: Ensure scans are clear and readable

**Ollama error**: Check if Ollama service is running

**Out of memory**: Reduce chunk size or use smaller embedding model

## ğŸ¤ Contributing

Contributions welcome! Feel free to:
- Report bugs
- Suggest features
- Submit pull requests

## ğŸ“„ License

MIT License - Free to use, modify, and distribute

## ğŸŒŸ Acknowledgments

- [Docling](https://github.com/DS4SD/docling) - Document processing
- [SentenceTransformers](https://www.sbert.net/) - Embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Ollama](https://ollama.com) - Local LLM runtime

## ğŸš€ Future Enhancements

- [ ] Web interface (Streamlit/Gradio)
- [ ] Multi-language support
- [ ] Image/diagram understanding
- [ ] Table extraction
- [ ] Batch processing
- [ ] Cloud storage integration

---

**Made with â¤ï¸ for learning and privacy**
